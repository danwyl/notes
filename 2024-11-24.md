## Sequence to Sequence

### Problem Formulation
Input: sequence of tokens \(x = (x_1, x_2, \ldots, x_{n})\)

Output: sequence of tokens $y = (y_1, y_2, \ldots, y_{m})$

such that $y$ is a transformation of the input sequence $x$.
Note that $n$ and $m$ can be different.

### Goal/Task
To do this, use the idea of conditional probability, where generally 

$P(y | x) = P(y_1|x) \cdot P(y_2|x, y_1) \ldots P(y_m|x, y_1, y_2, \dots, y_{m-1})$

$ = \prod_{i=1}^m P(y_i| x, y_1, y_2, \dots, y_{i-1})$

Breaking this down:

$P(y_1 | x)$ is simply the probability of the first token in the output sequence given the entire input sequence.

$P(y_2 | x, y_1)$ is the probability of the second token in the output sequence given the input sequence and the first token in the output sequence.

$P(y_m | x, y_1, y_2, \ldots, y_{m-1})$ is the probability of the last token in the output sequence given the input sequence and all the previous tokens in the output sequence.

That is all to say, the next token $y_i$ is conditioned on the input sequence $x$ and all of the output tokens that we have previously generated.

### Loss Function
Since we want to maximize $P(y|x)$, this is equivalent to minimizing the negative $-P(y|x)$. For a single example pair $x,y$, we have

$L_\theta(x,y) = -\sum_{i=1}^m \log P(y_i | x, y_1, y_2, \ldots, y_{i-1})$

where $\theta$ are the parameters of the model. Perhaps alternately (I could be thinking about this incorrectly) we want our model to assign a probability of 1 to the correct token:
$P(y_i | x, y_1, y_2, \ldots, y_{i-1}) = 1$, but we're actually going to get out some probability $ \hat y_i$ which is like the probability of the ground truth token $y_i$.
Then our loss function is like

$L_\theta(x,y) = -\sum_{i=1}^m \log \hat y_i(y_i)$

where $\hat y_i(y_i)$ is the probability of the ground truth token $y_i$.


Over all the training examples $\mathcal{S}$ with translation $T$ and source sentence $S$, we have

$L_\theta(\mathcal{S}) = -\sum_{(S,T) \in \mathcal{S}} \sum_{i=1}^m \log P(T_i | S, T_1, T_2, \ldots, T_{i-1}) = -\sum_{(S,T) \in \mathcal{S}} \log p (T | S)$

## Word Embeddings via Skip-gram and CBOW

In word embeddings, we want to represent words as vectors in some continuous space. We want to learn these embeddings based on the context that the words appear in. In general, we will have the context for some given word $w$ as:

$\mathrm{context}(w) = (w_{-k}, \ldots, w_{-1}, w_1, \ldots, w_k)$

for $w_{-k}, \ldots, w_{-1}, w_1, \ldots, w_k$ are the context words.

### Skip-gram
The skip-gram task is to predict the context words given a target word. 

For some word $w_c$ in the context of the target word $w_t$, 
we have some $P(w_c | w_t)$ that we want to maximize.

This is given as:

$P(w_c | w_t ) = \exp(u_{w_c}^T \cdot v_{w_t}) \cdot (\sum_{w \in V} \exp(v_{w}^T \cdot v_{w_t})^{-1})$

where $u_{w_c}$ is the embedding of the context word $w_c$ and $v_{w_t}$ is the embedding of the target word $w_t$. 

So it's like grabbing that similarity score between the context and the target, then normalizing it by the sum of the similarity scores between the target and all the other words in the vocabulary.

Then we want to take

$P(\mathrm{context}(w) | w) = \prod_{w_c \in \mathrm{context}(w)} P(w_c | w)$

which gives us the objective function (in log land):

$\frac{1}{T} \sum_{t=1}^T \sum_{-c \leq j \leq c, j \neq 0} \log P(w_{t+j} | w_t)$

where $T$ is the length of the training sequence, $c$ is the size of the context window, and $w_{t+j}$ is the $j$ th context word around the target word $w_t$.




### CBOW
The CBOW task is to predict the target word given the context words.

Thus for some target word $w_t$, we have some 

$P(w_t | w_{-k}, \ldots, w_{-1}, w_1, \ldots, w_k)$

To do this, we sum up (and average) the embeddings of the context words and then take the dot product with the target word embedding. This gives us 

$P(w_t | \mathrm{context}(w)) = \exp(u_{w_t}^T \cdot \frac{1}{2k} \sum_{w \in \mathrm{context}(w)} v_w) \cdot (\sum_{w \in V} \exp(v_{w}^T \cdot \frac{1}{2k} \sum_{w \in \mathrm{context}(w)} v_w)^{-1})$

where now our numerator is similarity between the target word and the average of the context word embeddings, and the denominator is our normalization term.


TODO: 
- unhappy with these loss function idea, expand and understand more
- negative sampling vs softmax
